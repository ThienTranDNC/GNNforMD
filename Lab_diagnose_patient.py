"""
Script ƒë·ªÉ ch·∫©n ƒëo√°n b·ªánh nh√¢n m·ªõi CH·ªà T·ª™ K·∫æT QU·∫¢ X√âT NGHI·ªÜM M√ÅU
S·ª≠ d·ª•ng Heterogeneous Graph + xAI (Explainable AI)
"""

import torch
from Lab_GNN import (
    load_graph_data, 
    create_hetero_patient_graphs,
    HeteroGNN_Diagnosis
)
from torch_geometric.data import HeteroData
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def make_default_values_multi():
    """
    H·ªì s∆° nhi·ªÅu b·ªánh g·ª£i √Ω: vi√™m ph·ªïi/nhi·ªÖm tr√πng (WBC‚Üë, NEUT‚Üë),
    thi·∫øu m√°u thi·∫øu s·∫Øt (HGB‚Üì, MCV‚Üì, MCH‚Üì, MCHC‚Üì), ti·ªÉu c·∫ßu ph·∫£n ·ª©ng (PLT‚Üë),
    k√®m tƒÉng nh·∫π eosinophil.
    """
    return {
        'RBC': 3.9,     # th·∫•p nh·∫π
        'HGB': 95,      # thi·∫øu m√°u
        'HCT': 0.31,    # theo HGB th·∫•p
        'MCV': 68,      # microcytic
        'MCH': 20.5,    # th·∫•p
        'MCHC': 300,    # th·∫•p
        'PLT': 480,     # thrombocytosis ph·∫£n ·ª©ng
        'WBC': 15.0,    # b·∫°ch c·∫ßu cao
        'NEUT': 78.0,   # tƒÉng trung t√≠nh
        'EO': 3.0,      # eosinophil h∆°i cao
        'BASO': 0.5,
        'MONO': 9.0,
        'LYMPH': 15.0,  # lympho t∆∞∆°ng ƒë·ªëi th·∫•p
        'MPV': 7.6,
        'PCT': 0.45,
        'PDW': 45.0
    }

def create_patient_graph_from_lab(lab_values, lab_to_idx, disease_to_idx):
    """
    T·∫°o HeteroData graph cho b·ªánh nh√¢n m·ªõi CH·ªà T·ª™ LAB VALUES
    """
    hetero_data = HeteroData()
    
    # Patient node
    hetero_data['patient'].x = torch.zeros(1, 16)
    
    # Lab nodes
    lab_features = list(lab_to_idx.keys())
    lab_node_features = torch.zeros(len(lab_features), 1)
    for lab_name, value in lab_values.items():
        if lab_name in lab_to_idx:
            idx = lab_to_idx[lab_name]
            lab_node_features[idx, 0] = value
    hetero_data['lab'].x = lab_node_features
    
    # Disease nodes (placeholder - model s·∫Ω t·ª± predict)
    hetero_data['disease'].x = torch.zeros(1, 2)
    
    # Edges
    patient_lab_edges = torch.tensor([[0] * len(lab_features), 
                                      list(range(len(lab_features)))], dtype=torch.long)
    hetero_data['patient', 'has_lab', 'lab'].edge_index = patient_lab_edges
    hetero_data['lab', 'rev_has_lab', 'patient'].edge_index = patient_lab_edges.flip([0])
    
    patient_disease_edges = torch.tensor([[0], [0]], dtype=torch.long)
    hetero_data['patient', 'has_disease', 'disease'].edge_index = patient_disease_edges
    hetero_data['disease', 'rev_has_disease', 'patient'].edge_index = patient_disease_edges.flip([0])
    
    return hetero_data

def compute_feature_importance_integrated_gradients(model, patient_graph, lab_to_idx, device, steps=50):
    """
    T√≠nh ƒë·ªô quan tr·ªçng c·ªßa t·ª´ng lab feature b·∫±ng Integrated Gradients
    """
    model.eval()
    patient_graph = patient_graph.to(device)
    
    # Baseline: all zeros
    baseline_graph = patient_graph.clone()
    baseline_graph['lab'].x = torch.zeros_like(baseline_graph['lab'].x)
    
    # Get predicted class
    with torch.no_grad():
        output = model(patient_graph)
        pred_class = output.argmax(dim=1).item()
    
    importances = {}
    lab_features = list(lab_to_idx.keys())
    
    for i, lab_name in enumerate(lab_features):
        # Integrated Gradients
        gradients = []
        for alpha in torch.linspace(0, 1, steps).to(device):
            interpolated_graph = patient_graph.clone()
            interpolated_graph['lab'].x = baseline_graph['lab'].x + alpha * (
                patient_graph['lab'].x - baseline_graph['lab'].x
            )
            interpolated_graph['lab'].x.requires_grad = True
            
            # Forward pass
            output = model(interpolated_graph)
            
            # Backward
            model.zero_grad()
            output[0, pred_class].backward(retain_graph=True)
            
            if interpolated_graph['lab'].x.grad is not None:
                gradients.append(interpolated_graph['lab'].x.grad[i].cpu().detach())
        
        if gradients:
            avg_gradient = torch.stack(gradients).mean(dim=0)
            feature_diff = (patient_graph['lab'].x[i] - baseline_graph['lab'].x[i]).cpu()
            importance = (avg_gradient * feature_diff).sum().item()
            importances[lab_name] = abs(importance)
    
    # Normalize
    total = sum(importances.values()) if importances else 1.0
    if total > 0:
        importances = {k: v/total for k, v in importances.items()}
    
    return importances, pred_class

def visualize_feature_importance(importances, lab_values, top_n=10):
    """
    V·∫Ω bi·ªÉu ƒë·ªì c√°c lab features quan tr·ªçng nh·∫•t
    """
    # Sort by importance
    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:top_n]
    features, scores = zip(*sorted_features)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. Bar chart - Feature Importance
    colors = plt.cm.RdYlGn(np.array(scores) / max(scores))
    bars = ax1.barh(range(len(features)), scores, color=colors, edgecolor='black', alpha=0.8)
    ax1.set_yticks(range(len(features)))
    ax1.set_yticklabels(features, fontsize=10)
    ax1.set_xlabel('Importance Score', fontsize=12, fontweight='bold')
    ax1.set_title(f'Top {top_n} Most Important Lab Features', fontsize=14, fontweight='bold')
    ax1.invert_yaxis()
    
    # Add value labels
    for i, (bar, score) in enumerate(zip(bars, scores)):
        ax1.text(score, i, f' {score*100:.1f}%', va='center', fontsize=9, fontweight='bold')
    
    # 2. Heatmap - Lab values vs Importance
    lab_data = []
    importance_data = []
    for feat in features:
        lab_data.append(lab_values.get(feat, 0))
        importance_data.append(importances[feat])
    
    # Normalize lab values for visualization
    lab_data_norm = (np.array(lab_data) - np.min(lab_data)) / (np.max(lab_data) - np.min(lab_data) + 1e-6)
    
    data_matrix = np.column_stack([lab_data_norm, importance_data])
    sns.heatmap(data_matrix, annot=[[f'{v:.2f}', f'{i*100:.1f}%'] 
                                     for v, i in zip(lab_data, importance_data)],
                fmt='', cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Score'},
                yticklabels=features, xticklabels=['Lab Value\n(normalized)', 'Importance\n(%)'])
    ax2.set_title('Lab Values vs Feature Importance', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('output/feature_importance.png', dpi=200, bbox_inches='tight')
    print(f"\n‚úì Saved: output/feature_importance.png")
    return fig

def generate_explanation_text(importances, results, lab_values):
    """
    T·∫°o vƒÉn b·∫£n gi·∫£i th√≠ch d·ªÖ hi·ªÉu cho b√°c sƒ©
    """
    predicted_disease = results['predicted_class']
    confidence = results['confidence']
    
    explanation = []
    explanation.append(f"\n{'='*70}")
    explanation.append(f"GI·∫¢I TH√çCH K·∫æT QU·∫¢ CH·∫®N ƒêO√ÅN (Explainable AI)")
    explanation.append(f"{'='*70}\n")
    
    # Main prediction
    explanation.append(f"üîç Ch·∫©n ƒëo√°n: {predicted_disease}")
    explanation.append(f"   ƒê·ªô tin c·∫≠y: {confidence*100:.2f}%\n")
    
    # Confidence interpretation
    if confidence > 0.8:
        conf_text = "R·∫§T CAO - Model r·∫•t ch·∫Øc ch·∫Øn"
    elif confidence > 0.6:
        conf_text = "CAO - Model kh√° ch·∫Øc ch·∫Øn"
    elif confidence > 0.4:
        conf_text = "TRUNG B√åNH - C·∫ßn c√¢n nh·∫Øc th√™m"
    else:
        conf_text = "TH·∫§P - C·∫ßn th√™m x√©t nghi·ªám"
    explanation.append(f"   ƒê√°nh gi√°: {conf_text}\n")
    
    # Top important features
    explanation.append(f"üìä C√ÅC CH·ªà S·ªê X√âT NGHI·ªÜM QUAN TR·ªåNG NH·∫§T:\n")
    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5]
    
    for i, (feature, importance) in enumerate(sorted_features, 1):
        bar = "‚ñà" * int(importance * 20)
        value = lab_values.get(feature, 0)
        explanation.append(f"   {i}. {feature:8s}: {bar} ({importance*100:.1f}%) | Gi√° tr·ªã: {value}")
    
    explanation.append(f"\n   ‚Üí C√°c ch·ªâ s·ªë n√†y c√≥ ·∫£nh h∆∞·ªüng l·ªõn nh·∫•t ƒë·∫øn ch·∫©n ƒëo√°n.\n")
    
    # Clinical interpretation
    explanation.append(f"ü©∫ DI·ªÑN GI·∫¢I L√ÇM S√ÄNG:\n")
    for feature, importance in sorted_features[:3]:
        value = lab_values.get(feature, 0)
        if feature == 'WBC' and value > 11:
            explanation.append(f"   ‚Ä¢ WBC cao ({value}) ‚Üí Nhi·ªÖm tr√πng/Vi√™m")
        elif feature == 'HGB' and value < 120:
            explanation.append(f"   ‚Ä¢ HGB th·∫•p ({value}) ‚Üí Thi·∫øu m√°u")
        elif feature == 'PLT' and value > 400:
            explanation.append(f"   ‚Ä¢ PLT cao ({value}) ‚Üí Ti·ªÉu c·∫ßu ph·∫£n ·ª©ng")
        elif feature == 'NEUT' and value > 70:
            explanation.append(f"   ‚Ä¢ NEUT cao ({value}%) ‚Üí Nhi·ªÖm khu·∫©n")
    
    explanation.append(f"\n{'='*70}\n")
    
    return "\n".join(explanation)

def main():
    print("\n" + "="*70)
    print("CH·∫®N ƒêO√ÅN B·ªÜNH NH√ÇN M·ªöI V·ªöI GI·∫¢I TH√çCH (xAI)")
    print("="*70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # Load model v√† mappings
    print("\n[1/5] ƒêang load model...")
    try:
        checkpoint = torch.load('best_hetero_model.pth', map_location=device)
        icd_to_idx = checkpoint['icd_to_idx']
        lab_to_idx = checkpoint['lab_to_idx']
        disease_to_idx = checkpoint['disease_to_idx']
        hidden_dim = checkpoint['hidden_dim']
        output_dim = checkpoint['num_classes']
        
        model = HeteroGNN_Diagnosis(hidden_dim, output_dim, dropout=0.3).to(device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"‚úì Model loaded! Classes: {output_dim}")
        
    except FileNotFoundError:
        print("‚úó File 'best_hetero_model.pth' kh√¥ng t·ªìn t·∫°i!")
        print("üëâ Ch·∫°y: python Lab_GNN.py")
        return
    except KeyError:
        print("‚úó Model c≈© thi·∫øu mappings!")
        print("üëâ Train l·∫°i: python Lab_GNN.py")
        return
    
    # T√≠nh lab statistics
    print("\n[2/5] ƒêang t√≠nh statistics...")
    df = load_graph_data('data/Lab_graph_edge_list.csv')
    lab_features = list(lab_to_idx.keys())
    all_lab_values = {feat: [] for feat in lab_features}
    
    sample_patients = df['source'].unique()[:100]
    for patient_id in sample_patients:
        patient_data = df[df['source'] == patient_id]
        lab_data = patient_data[patient_data['edge_type'] == 'Have_Lab']
        for _, row in lab_data.iterrows():
            feat_name = row['target']
            if feat_name in all_lab_values:
                try:
                    all_lab_values[feat_name].append(float(row['weight']))
                except:
                    pass
    
    lab_stats = {}
    for feat in lab_features:
        if len(all_lab_values[feat]) > 0:
            lab_stats[feat] = {
                'mean': np.mean(all_lab_values[feat]),
                'std': np.std(all_lab_values[feat]) + 1e-6
            }
        else:
            lab_stats[feat] = {'mean': 0, 'std': 1}
    
    print(f"‚úì Statistics ready")
    
    # Patient info
    print("\n[3/5] Th√¥ng tin b·ªánh nh√¢n...")
    
    patient_info = {
        'ID': "NEW_PATIENT_DEMO",
        'Tu·ªïi': "45",
        'Gi·ªõi t√≠nh': "Nam",
        'Tri·ªáu ch·ª©ng': "S·ªët cao, ho, kh√≥ th·ªü"
    }
    
    print(f"Patient ID: {patient_info['ID']}")
    print(f"Tu·ªïi: {patient_info['Tu·ªïi']}")
    print(f"Gi·ªõi t√≠nh: {patient_info['Gi·ªõi t√≠nh']}")
    print(f"Tri·ªáu ch·ª©ng: {patient_info['Tri·ªáu ch·ª©ng']}")
    
    # S·ª≠ d·ª•ng gi√° tr·ªã x√©t nghi·ªám m·∫∑c ƒë·ªãnh (multi-disease profile)
    print("\nS·ª≠ d·ª•ng k·∫øt qu·∫£ x√©t nghi·ªám m√°u m·∫∑c ƒë·ªãnh (ƒëa b·ªánh: nhi·ªÖm tr√πng + thi·∫øu m√°u thi·∫øu s·∫Øt + TC ph·∫£n ·ª©ng):")
    default_values = make_default_values_multi()
    
    lab_values = {}
    for lab_name in lab_features:
        lab_values[lab_name] = default_values.get(lab_name, 0)
    
    # Hi·ªÉn th·ªã c√°c gi√° tr·ªã x√©t nghi·ªám quan tr·ªçng
    print("C√°c ch·ªâ s·ªë ch√≠nh:")
    important_labs = ['WBC', 'NEUT', 'LYMPH', 'RBC', 'HGB', 'PLT']
    for lab_name in important_labs:
        if lab_name in lab_values:
            print(f"  {lab_name}: {lab_values[lab_name]}")
    print(f"  ... (v√† {len(lab_values) - len(important_labs)} ch·ªâ s·ªë kh√°c)")
    
    # Diagnosis v·ªõi xAI
    print("\n[4/5] ƒêang ch·∫©n ƒëo√°n V·ªöI GI·∫¢I TH√çCH (xAI)...")
    patient_graph = create_patient_graph_from_lab(lab_values, lab_to_idx, disease_to_idx)
    patient_graph = patient_graph.to(device)
    
    model.eval()
    with torch.no_grad():
        log_probs = model(patient_graph)
        probs = torch.exp(log_probs).squeeze(0)
        
        # Top 5 predictions
        top_probs, top_indices = torch.topk(probs, k=5)
        idx_to_icd = {idx: code for code, idx in icd_to_idx.items()}
        
        print("\nK·∫æT QU·∫¢ CH·∫®N ƒêO√ÅN:")
        print("-"*70)
        for i, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):
            disease = idx_to_icd[int(idx)]
            print(f"{i}. {disease}: {float(prob)*100:.2f}%")
        print("-"*70)
        
        results = {
            'predicted_class': idx_to_icd[int(top_indices[0])],
            'confidence': float(top_probs[0])
        }
    
    # xAI - Feature Importance
    print("\n[5/5] ƒêang t√≠nh to√°n ƒë·ªô quan tr·ªçng c·ªßa c√°c ch·ªâ s·ªë (xAI)...")
    importances, pred_class = compute_feature_importance_integrated_gradients(
        model, patient_graph, lab_to_idx, device, steps=30
    )
    
    # Generate explanation
    explanation_text = generate_explanation_text(importances, results, lab_values)
    print(explanation_text)
    
    # Visualize
    import os
    os.makedirs('output', exist_ok=True)
    visualize_feature_importance(importances, lab_values, top_n=10)
    
    print("\n‚úÖ Ho√†n t·∫•t ch·∫©n ƒëo√°n v·ªõi gi·∫£i th√≠ch!")

if __name__ == "__main__":
    main()
