# gnn_diag_minimal.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import HeteroConv, SAGEConv, GATConv, Linear
from torch_geometric.loader import DataLoader
from torch_geometric.data import HeteroData
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import os

# ===========================
# 1. ƒê·ªåC V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU
# ===========================

def load_graph_data(filepath='data/Lab_graph_edge_list.csv'):
    """
    ƒê·ªçc d·ªØ li·ªáu ƒë·ªì th·ªã t·ª´ file CSV
    Format: source | target | weight | edge_type
    """
    df = pd.read_csv(filepath, sep=' ', header=None, 
                     names=['source', 'target', 'weight', 'edge_type'])
    return df

def create_hetero_patient_graphs(df):
    """
    T·∫°o Heterogeneous Graph cho t·ª´ng b·ªánh nh√¢n
    3 lo·∫°i node:
    - Patient: 1 node (b·ªánh nh√¢n)
    - Lab: 16 nodes (c√°c ch·ªâ s·ªë x√©t nghi·ªám)
    - Disease: M nodes (T·∫§T C·∫¢ c√°c b·ªánh)
    Tr·ªçng s·ªë c·∫°nh (patient -> disease) l·∫•y t·ª´ c·ªôt 'weight'
    """
    lab_features = ['RBC', 'HGB', 'HCT', 'MCV', 'MCH', 'MCHC', 'PLT', 'WBC', 
                    'NEUT', 'EO', 'BASO', 'MONO', 'LYMPH', 'MPV', 'PCT', 'PDW']
    
    print(f"Edge types trong d·ªØ li·ªáu: {df['edge_type'].unique()}")
    disease_edges = df[df['edge_type'] == 'Have_Disease']
    print(f"S·ªë d√≤ng Have_Disease: {len(disease_edges)}")
    
    all_diseases = sorted(disease_edges['target'].unique())
    disease_to_idx = {code: idx for idx, code in enumerate(all_diseases)}
    # S·ª≠ d·ª•ng chung disease_to_idx cho c·∫£ label v√† node mapping
    icd_to_idx = disease_to_idx
    
    print(f"T·ªïng s·ªë b·ªánh: {len(all_diseases)}")
    if len(all_diseases) > 0:
        print(f"V√≠ d·ª• 5 b·ªánh ƒë·∫ßu: {all_diseases[:5]}")
    
    lab_to_idx = {feat: idx for idx, feat in enumerate(lab_features)}
    
    graphs = []
    labels = []
    patient_ids_list = []
    
    unique_patients = df['source'].unique()
    print(f"S·ªë l∆∞·ª£ng patients: {len(unique_patients)}")
    
    for patient_id in unique_patients:
        patient_data = df[df['source'] == patient_id]
        patient_diseases = patient_data[patient_data['edge_type'] == 'Have_Disease']
        if len(patient_diseases) == 0:
            continue
        
        primary_disease = patient_diseases.iloc[0]['target']
        if primary_disease not in icd_to_idx:
            print(f"Warning: Disease {primary_disease} not in icd_to_idx")
            continue
        label = icd_to_idx[primary_disease]
        
        hetero_data = HeteroData()
        hetero_data['patient'].x = torch.zeros(1, 16)
        hetero_data['patient'].y = torch.tensor([label], dtype=torch.long)
        
        lab_data = patient_data[patient_data['edge_type'] == 'Have_Lab']
        lab_node_features = torch.zeros(len(lab_features), 1)
        for _, row in lab_data.iterrows():
            feat_name = row['target']
            if feat_name in lab_to_idx:
                idx = lab_to_idx[feat_name]
                lab_node_features[idx, 0] = float(row['weight'])
        hetero_data['lab'].x = lab_node_features
        
        disease_list = []
        disease_weights = []
        for _, row in patient_diseases.iterrows():
            disease_code = row['target']
            if disease_code in disease_to_idx:
                disease_list.append(disease_to_idx[disease_code])
                try:
                    weight = float(row['weight'])
                except:
                    weight = 1.0
                disease_weights.append(weight)
        
        num_diseases = max(len(disease_list), 1)
        disease_node_features = torch.zeros(num_diseases, 2)
        for idx, (disease_idx, weight) in enumerate(zip(disease_list, disease_weights)):
            disease_node_features[idx, 0] = disease_idx / len(all_diseases)
            disease_node_features[idx, 1] = weight
        hetero_data['disease'].x = disease_node_features
        
        patient_lab_edges = torch.tensor([[0] * len(lab_features), 
                                          list(range(len(lab_features)))], dtype=torch.long)
        hetero_data['patient', 'has_lab', 'lab'].edge_index = patient_lab_edges
        
        patient_disease_edges = torch.tensor([[0] * num_diseases, 
                                             list(range(num_diseases))], dtype=torch.long)
        hetero_data['patient', 'has_disease', 'disease'].edge_index = patient_disease_edges
        hetero_data['patient', 'has_disease', 'disease'].edge_attr = torch.tensor(
            disease_weights, dtype=torch.float).unsqueeze(1)
        
        hetero_data['lab', 'rev_has_lab', 'patient'].edge_index = patient_lab_edges.flip([0])
        hetero_data['disease', 'rev_has_disease', 'patient'].edge_index = patient_disease_edges.flip([0])
        hetero_data['disease', 'rev_has_disease', 'patient'].edge_attr = torch.tensor(
            disease_weights, dtype=torch.float).unsqueeze(1)
        
        graphs.append(hetero_data)
        labels.append(label)
        patient_ids_list.append(patient_id)
    
    print(f"ƒê√£ t·∫°o {len(graphs)} graphs")
    
    return graphs, labels, patient_ids_list, icd_to_idx, lab_to_idx, disease_to_idx

# ===========================
# 2. ƒê·ªäNH NGHƒ®A M√î H√åNH HETERO GNN
# ===========================

class HeteroGNN_Diagnosis(nn.Module):
    """
    Heterogeneous Graph Neural Network cho ch·∫©n ƒëo√°n b·ªánh
    3 lo·∫°i node: Patient, Lab, Disease
    """
    def __init__(self, hidden_dim, output_dim, dropout=0.5):
        super(HeteroGNN_Diagnosis, self).__init__()
        
        # Input projections
        self.patient_lin = Linear(16, hidden_dim)
        self.lab_lin = Linear(1, hidden_dim)
        self.disease_lin = Linear(2, hidden_dim)
        
        # Heterogeneous Graph Convolution Layers
        self.conv1 = HeteroConv({
            ('patient', 'has_lab', 'lab'): SAGEConv(hidden_dim, hidden_dim),
            ('patient', 'has_disease', 'disease'): SAGEConv(hidden_dim, hidden_dim),
            ('lab', 'rev_has_lab', 'patient'): SAGEConv(hidden_dim, hidden_dim),
            ('disease', 'rev_has_disease', 'patient'): SAGEConv(hidden_dim, hidden_dim),
        }, aggr='sum')
        
        self.conv2 = HeteroConv({
            ('patient', 'has_lab', 'lab'): SAGEConv(hidden_dim, hidden_dim),
            ('patient', 'has_disease', 'disease'): SAGEConv(hidden_dim, hidden_dim),
            ('lab', 'rev_has_lab', 'patient'): SAGEConv(hidden_dim, hidden_dim),
            ('disease', 'rev_has_disease', 'patient'): SAGEConv(hidden_dim, hidden_dim),
        }, aggr='sum')
        
        # Batch normalization
        self.bn1 = nn.ModuleDict({
            'patient': nn.BatchNorm1d(hidden_dim),
            'lab': nn.BatchNorm1d(hidden_dim),
            'disease': nn.BatchNorm1d(hidden_dim)
        })
        
        self.bn2 = nn.ModuleDict({
            'patient': nn.BatchNorm1d(hidden_dim),
            'lab': nn.BatchNorm1d(hidden_dim),
            'disease': nn.BatchNorm1d(hidden_dim)
        })
        
        # Classification head
        self.fc1 = Linear(hidden_dim, hidden_dim // 2)
        self.fc2 = Linear(hidden_dim // 2, output_dim)
        
        self.dropout = dropout
    
    def forward(self, data):
        # Project input features
        x_dict = {
            'patient': self.patient_lin(data['patient'].x),
            'lab': self.lab_lin(data['lab'].x),
            'disease': self.disease_lin(data['disease'].x)
        }
        
        # Layer 1
        x_dict = self.conv1(x_dict, data.edge_index_dict)
        x_dict = {key: self.bn1[key](x) for key, x in x_dict.items()}
        x_dict = {key: F.relu(x) for key, x in x_dict.items()}
        x_dict = {key: F.dropout(x, p=self.dropout, training=self.training) for key, x in x_dict.items()}
        
        # Layer 2
        x_dict = self.conv2(x_dict, data.edge_index_dict)
        x_dict = {key: self.bn2[key](x) for key, x in x_dict.items()}
        x_dict = {key: F.relu(x) for key, x in x_dict.items()}
        
        # Patient node embedding cho classification
        patient_emb = x_dict['patient']
        
        # Classification
        x = F.relu(self.fc1(patient_emb))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc2(x)
        
        return F.log_softmax(x, dim=1)

# ===========================
# 3. TRAINING V√Ä EVALUATION
# ===========================

def train(model, loader, optimizer, criterion, device):
    """Training loop"""
    model.train()
    total_loss = 0
    correct = 0
    total_samples = 0
    
    for data in loader:
        data = data.to(device)
        optimizer.zero_grad()
        
        out = model(data)
        # Fix: L·∫•y y t·ª´ patient node
        labels = data['patient'].y
        loss = criterion(out, labels)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * labels.size(0)
        pred = out.argmax(dim=1)
        correct += (pred == labels).sum().item()
        total_samples += labels.size(0)
    
    return total_loss / total_samples, correct / total_samples

def evaluate(model, loader, criterion, device):
    """Evaluation loop"""
    model.eval()
    total_loss = 0
    correct = 0
    all_preds = []
    all_labels = []
    total_samples = 0
    
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            out = model(data)
            # Fix: L·∫•y y t·ª´ patient node
            labels = data['patient'].y
            loss = criterion(out, labels)
            
            total_loss += loss.item() * labels.size(0)
            pred = out.argmax(dim=1)
            correct += (pred == labels).sum().item()
            total_samples += labels.size(0)
            
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    return total_loss / total_samples, correct / total_samples, all_preds, all_labels

def plot_training_history(train_losses, train_accs, val_losses, val_accs):
    """V·∫Ω bi·ªÉu ƒë·ªì loss v√† accuracy"""
    os.makedirs('output', exist_ok=True)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Loss
    ax1.plot(train_losses, label='Train Loss', linewidth=2, marker='o', markersize=4)
    ax1.plot(val_losses, label='Val Loss', linewidth=2, marker='s', markersize=4)
    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    
    # Accuracy
    ax2.plot(train_accs, label='Train Acc', linewidth=2, marker='o', markersize=4)
    ax2.plot(val_accs, label='Val Acc', linewidth=2, marker='s', markersize=4)
    ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('output/training_history.png', dpi=300, bbox_inches='tight')
    print(f"\n‚úì Saved: output/training_history.png")
    plt.close()

def plot_confusion_matrix(y_true, y_pred, icd_to_idx):
    """V·∫Ω confusion matrix"""
    os.makedirs('output', exist_ok=True)
    
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=sorted(icd_to_idx.keys()),
                yticklabels=sorted(icd_to_idx.keys()),
                cbar_kws={'label': 'Count'})
    plt.xlabel('Predicted', fontsize=12, fontweight='bold')
    plt.ylabel('Actual', fontsize=12, fontweight='bold')
    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('output/confusion_matrix.png', dpi=300, bbox_inches='tight')
    print(f"‚úì Saved: output/confusion_matrix.png")
    plt.close()

# ===========================
# 4. CH·∫†Y TO√ÄN B·ªò QU√Å TR√åNH
# ===========================
if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # Load d·ªØ li·ªáu
    print("ƒêang load d·ªØ li·ªáu...")
    df = load_graph_data('data/Lab_graph_edge_list.csv')
    
    # DEBUG: In th√¥ng tin v·ªÅ dataframe
    print(f"\nShape c·ªßa dataframe: {df.shape}")
    print(f"C√°c c·ªôt: {df.columns.tolist()}")
    print(f"\n5 d√≤ng ƒë·∫ßu:")
    print(df.head())
    
    graphs, labels, patient_ids_list, icd_to_idx, lab_to_idx, disease_to_idx = create_hetero_patient_graphs(df)
    
    # Ki·ªÉm tra s·ªë l∆∞·ª£ng graphs
    if len(graphs) == 0:
        print("\n‚ùå ERROR: Kh√¥ng c√≥ graph n√†o ƒë∆∞·ª£c t·∫°o!")
        print("Ki·ªÉm tra:")
        print("1. Edge_type 'Have_Disease' c√≥ t·ªìn t·∫°i kh√¥ng?")
        print("2. Edge_type 'Have_Lab' c√≥ t·ªìn t·∫°i kh√¥ng?")
        print("3. D·ªØ li·ªáu c√≥ ƒë√∫ng format: source | target | weight | edge_type kh√¥ng?")
        exit(1)
    
    print(f"\n‚úì ƒê√£ t·∫°o {len(graphs)} graphs th√†nh c√¥ng!")
    
    # ===== L·ªåC B·ªé C√ÅC CLASS C√ì √çT H∆†N 2 SAMPLES =====
    label_counts = Counter(labels)
    
    print(f"\nüìä Ph√¢n ph·ªëi class tr∆∞·ªõc khi l·ªçc:")
    for disease_code, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):
        disease_name = [k for k, v in icd_to_idx.items() if v == disease_code][0]
        print(f"  {disease_name}: {count} samples")
    
    # L·ªçc b·ªè class c√≥ < 2 samples
    min_samples = 2
    valid_indices = []
    for i, label in enumerate(labels):
        if label_counts[label] >= min_samples:
            valid_indices.append(i)
    
    # C·∫≠p nh·∫≠t danh s√°ch
    filtered_graphs = [graphs[i] for i in valid_indices]
    filtered_labels = [labels[i] for i in valid_indices]
    filtered_patient_ids = [patient_ids_list[i] for i in valid_indices]
    
    # T·∫°o l·∫°i icd_to_idx ch·ªâ v·ªõi c√°c class c√≤n l·∫°i
    unique_filtered_labels = sorted(set(filtered_labels))
    old_to_new_label = {old_label: new_label for new_label, old_label in enumerate(unique_filtered_labels)}
    
    # Remap labels sang 0, 1, 2, ...
    remapped_labels = [old_to_new_label[label] for label in filtered_labels]
    
    # C·∫≠p nh·∫≠t y trong graphs
    for graph, new_label in zip(filtered_graphs, remapped_labels):
        graph['patient'].y = torch.tensor([new_label], dtype=torch.long)
    
    # T·∫°o l·∫°i icd_to_idx mapping
    idx_to_disease_old = {v: k for k, v in icd_to_idx.items()}
    new_icd_to_idx = {}
    for new_idx, old_idx in enumerate(unique_filtered_labels):
        disease_code = idx_to_disease_old[old_idx]
        new_icd_to_idx[disease_code] = new_idx
    
    print(f"\nüîç Sau khi l·ªçc:")
    print(f"  Lo·∫°i b·ªè: {len(graphs) - len(filtered_graphs)} samples")
    print(f"  C√≤n l·∫°i: {len(filtered_graphs)} samples v·ªõi {len(new_icd_to_idx)} classes")
    
    print(f"\nüìä Ph√¢n ph·ªëi class sau khi l·ªçc:")
    remapped_label_counts = Counter(remapped_labels)
    for new_label, count in sorted(remapped_label_counts.items()):
        disease_code = [k for k, v in new_icd_to_idx.items() if v == new_label][0]
        print(f"  {disease_code}: {count} samples")
    
    # Ki·ªÉm tra l·∫°i
    if len(filtered_graphs) < 10:
        print(f"\n‚ùå ERROR: Qu√° √≠t samples ({len(filtered_graphs)}) sau khi l·ªçc!")
        exit(1)
    
    # S·ª≠ d·ª•ng filtered data
    graphs = filtered_graphs
    labels = remapped_labels
    patient_ids_list = filtered_patient_ids
    icd_to_idx = new_icd_to_idx
    
    # Chia t·∫≠p train/val/test
    try:
        train_graphs, test_graphs, train_labels, test_labels = train_test_split(
            graphs, labels, test_size=0.2, random_state=42, stratify=labels)
        train_graphs, val_graphs, train_labels, val_labels = train_test_split(
            train_graphs, train_labels, test_size=0.1, random_state=42, stratify=train_labels)
    except ValueError as e:
        print(f"\n‚ö†Ô∏è  Kh√¥ng th·ªÉ stratified split: {e}")
        print("  S·ª≠ d·ª•ng random split thay th·∫ø...")
        train_graphs, test_graphs, train_labels, test_labels = train_test_split(
            graphs, labels, test_size=0.2, random_state=42)
        train_graphs, val_graphs, train_labels, val_labels = train_test_split(
            train_graphs, train_labels, test_size=0.1, random_state=42)
    
    print(f"\nS·ªë l∆∞·ª£ng graph - Train: {len(train_graphs)}, Val: {len(val_graphs)}, Test: {len(test_graphs)}")
    
    # T·∫°o DataLoader
    batch_size = 32
    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_graphs, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)
    
    # Kh·ªüi t·∫°o m√¥ h√¨nh
    hidden_dim = 128
    output_dim = len(icd_to_idx)
    model = HeteroGNN_Diagnosis(hidden_dim, output_dim, dropout=0.3).to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)
    criterion = nn.NLLLoss()
    
    # Training loop
    num_epochs = 50
    best_val_loss = float('inf')
    best_val_acc = 0.0
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    patience = 50  # S·ªë epoch kh√¥ng c·∫£i thi·ªán tr∆∞·ªõc khi d·ª´ng
    patience_counter = 0

    for epoch in range(1, num_epochs + 1):
        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)
        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)    
        
        print(f"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save({
                'model_state_dict': model.state_dict(),
                'icd_to_idx': icd_to_idx,
                'lab_to_idx': lab_to_idx,
                'disease_to_idx': disease_to_idx,
                'num_classes': output_dim,
                'hidden_dim': hidden_dim,
                'train_losses': train_losses,
                'train_accs': train_accs,
                'val_losses': val_losses,
                'val_accs': val_accs
            }, 'output/best_hetero_model.pth')
            print("‚úì Model saved!")
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= patience:
            print(f"Early stopping: Kh√¥ng c·∫£i thi·ªán val_acc sau {patience} epoch li√™n ti·∫øp.")
            break

    # Plot training history
    plot_training_history(train_losses, train_accs, val_losses, val_accs)
    
    # Evaluate on test set
    test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, device)
    print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}")
    
    # Plot confusion matrix
    plot_confusion_matrix(test_labels, test_preds, icd_to_idx)